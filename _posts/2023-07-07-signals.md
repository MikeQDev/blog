---
layout: post
title:  "Telemetry Signals"
categories: Observability
tags: Signals Observability High-level
author: Mike
---

This blog post outlines the three main telemetry signals as of July 2023 (per the [OpenTelemetry spec](https://opentelemetry.io/docs/concepts/signals/)) and how they're used in harmony to achieve operational excellence (and more!)

## Signals

### Metrics
Metrics have been traditionally used to monitor infrastructure (how much disk space is remaining? How is my CPU utilization? etc.), and they still are. As we move more towards cloud-native components, we (as application engineers) don't need to worry as much about these underlying infrastructure metrics because our cloud platform maintains infrastructure for us (think serverless). Monitoring cloud infrastructure metrics is fairly redundant and usually results in noise, which encourages alert fatigue to engineers on your product team. (More on this in a future blog post.)

That's not to say we shouldn't use metrics at all. Instead of leveraging infrastructure metrics, we should move up a layer and capture metrics at the application level. More specifically, capture application metrics that are critical to the end users journey. Is the checkout page taking too long to load? Are payments failing to be accepted? Both of those scenarios would negatively affect the business - well at the core, it'll affect your customers, who are incidentally the lifeblood of our business.

<!-- Does that mean we should alert each time a request is slow, or for each error a user experiences? Heck no! Doing so would result in alert fatigue and distractions from higher priority initiatives. In a world of modern architectures and distributed systems, things are constantly failing (a lot of them are hidden from us thanks to fault-tolerance). We must accept failure as normal, and be intentional about what we alert on. -->

I'll ramble on in a separate blog post about metrics, SLOs (Service Level Objectives), and alerting, but the point to take away for now is it's not feasible to alert on each bad metric emitted. Instead, we must aggregate metrics and calculate if our service is at risk of being unreliable. Once these aggregate metrics (Service Level Indicators/SLIs) show that reliability is in danger (and an actionable solution is feasible), the engineer(s) on-call are notified to restore the service to health so business can continue as usual.

### Distributed Traces
Once an on-call engineer is notified that something is going wrong, they must identify *why and where* the problem is occurring before fully resolving the issue. Depending on the alert message, an engineer is likely to log into their observability backend tool to start "asking questions", typically through dashboards or saved queries. A chart visual might indicate a spike in latency for certain requests. The engineer might filter for all requests that returned an error. Whatever it may be, after some investigation, the engineer can see exactly *where* in the system something is going wrong, and start to identify attributes in the telemetry data that may reveal the root cause of the issue.<!-- While [most outages are caused by some kind of change](https://sre.google/sre-book/introduction/#:~:text=most%20outages%20are%20caused%20by%20some%20kind%20of%20change)-->

To understand how distributed tracing enables this sort of insight, it's important to understand the definition and anatomy of a trace. A distributed trace, simply put, is a representation of a request flowing end-to-end (across each component) within (and sometimes across) a system(s). Observability backends commonly display traces in a waterfally view - if you Google image search 'distributed tracing waterfall', you'll see what I mean. Each block in the distributed tracing chart is a span (one unit of work), which contains information about it's `span_id`, `span_parent`, `start_time`, `duration`, and corresponding `trace_id` at a minimum. (Note, `trace_id`, `span_parent`, `trace_state`, and `trace_flags` are considered span "context" and propagated via request headers when crossing boundaries in a system to "stitch" the spans together in the observability backend). Additional values should be included in the span as well, such as data about the emitting component (e.g.: resource name, region, etc.) and the request itself (e.g.: payload, userId, etc.). Attaching useful attributes to spans is what enables us to identify patterns (and anomalies) to better understand erroneous requests.

Implementing distributed tracing is fairly simple. "Auto-instrumentation" will [automatically instrument popular libraries](https://opentelemetry.io/ecosystem/registry/?component=instrumentation&language=js) (e.g.: http clients, database drivers, etc.) within your application so you can get traces (and other signals) for those libraries without much additional work. Auto-instrumentation should then be augmented with manual instrumentation to provide a more detailed view of the requests that flow through your system (particularly, through the custom business logic that isn't captured with pre-instrumentated libraries).

### Logs
When distributed traces don't provide enough details to fully diagnose an issue - or if distributed traces aren't present at all - logs are our last resort. Traditionally, sifting through logs required SSHing into a server (sometimes multiple servers) and grepping the log file for issues. To make this process easier, log-forwarding agent software was installed on machines to automatically forward logs to a centralized logging tool (e.g. ELK, Splunk, GrayLog, etc.). It's important that resource metadata describing where each log came from is indicated for each log record contained in the centralized logging tool, otherwise it'd be difficult to pinpoint issues back to the affected machine. 

In addition to having resource metadata in our logs, it's important to also include rich context about the request in a structured logging format. Why? Because structured data is what machines best understand. Single-lined, unstructured logs are fine for human readability, but as systems scale, this simple approach of a human reading logs line-by-line does not. With the incredible processing power machines offer, it'd be silly not to write structured logs that allow us to leverage machines to effortlessly slice and dice our data to find patterns and anomalies. Proper system insight comes from a combination of human intuition and the processing power machines offer to make use of big data.

With systems continuously surpassing record transactional throughput (thousands of transactions per second (TPS) are common, and some systems even have millions of TPS), it's near impossible to pinpoint as issue when trying to correlate metrics with logs via timestamp. Another benefit of context-rich, structured logs is that some logging libraries automatically integrate with telemetry instrumentation (e.g.: OpenTelemetry) to automatically include the corresponding requests `trace_id` for fast discovery of useful logs.

## Using signals in harmony
- **Metrics** feed SLIs/SLOs, which alert us when our service is unreliable
- We view the **traces** to figure out where in the system things are going wrong, and ideally why they are going wrong
- Review structured **logs** to get more details about what's going on within a component
- Once the root of the issue is identified, a fix is put into place and deployed to production

Using this approach enables rapid Mean-Time To Resolution (MTTR) = happier customers = successful business

To reduce cognitive load of the engineer leveraging these signals, two things can be done. The first is having all signals stored in a single observability backend -- having all telemetry data in a single tool is much easier than than hopping across tools while trying to maintain context. Secondly, all of these signals should easily correlate with each other via `trace_id` -- where appropriate, metrics should have 'exemplars' containing `trace_id`, and structured logs should also contain `trace_id`.

## Recommendations
### Metrics
- Leverage what you can out of the box where it makes sense (e.g.: API Gateway response codes, latency, etc.)
- Don't page on-callers based on raw metrics (things constantly fail in complex systems). Rather, alert on meaningful (and actionable) events, such as SLOs being in danger of being breached (more to come on this in a separate blog post)
- Emit custom (application) metrics that are meaningful for end-user journeys. Just be cautious of creating metrics with high cardinality dimensions, as this can get expensive

### Distributed Traces
- An absolute must for distributed systems/modern architectures
- OpenTelemetry is the de facto standard for tracing, and supports most popular languages
- Leverage auto-instrumentation, and augment with manual instrumentation for a better view of your systems

### Logs
- Try to avoid them? Instead, include plenty of rich event data in your distributed tracing spans
- Include `trace_id` as needed (instrumentation should do this automatically)
- String-concatenated messages aren't great for machine usability. Rather than having a `{"message": "User 32101 has logged in"}`, we should be logging in a high-dimensional way such as `{"message": "User has logged in", Attributes: { "userId": 32101 } }`. This makes slicing and dicing our log data much easier

## Outside of operational excellence
You've probably heard that "data is beautiful"; telemetry data absolutely doesn't need to be used *just* for systems diagnosis. Having telemetry emitted and accessible by anyone on your team (from engineers to product managers to business stakeholders), brings everyone closer to the lens of production. Telemetry data can be used for business intelligence ([How] Are our users using a new feature? Are all of our users having a satisfactory experience using our product? How should we prioritize our work in the future? etc.). Of course, the data points needed to ask some of these questions are business specific - something you wouldn't get from auto-instrumentation alone ;)

* content
{:toc}

